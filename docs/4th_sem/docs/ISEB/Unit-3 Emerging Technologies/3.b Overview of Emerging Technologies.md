# Emerging Technology

Emerging technology is a terminology that is typically used to describe a new technology, but it can also refer to the ongoing evolution of an existing technology. When used in different contexts, such as in the media, business, science, or educational domains, it can signify slightly different things. 

Usually reserved for technologies that are causing, or are expected to create, substantial societal or economic repercussions, the word is widely used to describe technologies that are actively development or that are projected to be available within the next five to ten years.

New potential and legal concerns have resulted from emerging digital technologies, especially in relation to copyrights, trademarks, patents, royalties, andÂ licensing. 

The federal government, impacted sectors, and public interest advocacy groups have taken (and still are taking) steps to design suitable safeguards and provide legal assurance to digital technology enterprises, copyright holders, the public, and other interested parties.

## Emerging Technology Trends 

### 1. Artificial intelligence 
   The creation of computer systems capable of carrying out activities requiring human intelligence is known as artificial intelligence, or AI. Artificial Intelligence assists with data processing, pattern recognition, and decision-making. Methods including robotics, computer vision, natural language processing, and machine learning can be used to do this. Learning, reasoning, perception, problem solving, data processing, and language comprehension are just a few of the skills that are included in AI. The ultimate goal of artificial intelligence is to build machines that can mimic human talents and perform a variety of activities more accurately and efficiently.

  ### 2. Cloud Services 
  A wide variety of services that are provided on demand to businesses and consumers via the internet are referred to as cloud services. These services are made to give users quick, inexpensive access to resources and applications without requiring hardware or internal infrastructure.

   As humongous data is generated daily, it becomes a challenge to find solutions for low-cost storage and cheap power. 
  This is where cloud computing and services come as a saviour. 
   Cloud services aim at storing large amounts of data for a low cost to efficiently tackle the issues encountered regarding storage in data science.

   ### 3. Augmented Reality (AR) And Virtual Reality (VR)
   
  - AR creates an artificial environment by combining components of the physical world with the digital world. AR-enabled desktop and mobile applications that seamlessly integrate digital elements into the physical world. AR stands for Augment Reality in its full form.

      For instance, augmented reality technology makes it possible to see score overlays during live sporting events and to see 3D images, emails, and texts.
       Augmented Reality (AR) employs computer vision, mapping, and depth sensing to present relevant content to users. With the use of this feature, cameras can gather, transmit, and process data in order to display digital material that is relevant to whatever screen a person is viewing.

  - A computer-generated simulation of a different reality or universe is known as virtual reality, or VR. Video games and 3D films both use it. Using computers and sensory apparatus like gloves and headsets, it helps to "immerse" the spectator in realistic-looking simulations.

       Virtual reality is primarily concerned with vision simulation. The user must place a virtual reality headset screen in front of their eyes. Consequently, removing all communication with the outside world. Two lenses are positioned between the screen in virtual reality. The user must adjust their eyes according to each eye's unique movement and placement. An HDMI cable that is connected to a computer or smartphone can be used to render the images on the screen.

  ### 4. Quantum Computing 
 
  Quantum computing is still relatively new. Complex calculations should be completed by quantum computers in a matter of seconds. 
 These computations take too long for computers of today to complete; they would likely take at least a century.
 Large corporations like Google have already started investigating quantum computing, which entails storing a significant amount of data in quantum bits, or qubits, so they can quickly answer complicated computations.
 But for the time being, it is not a practical choice. By 2022, quantum computing should become the main attraction.

### 5. Internet Of Things ( IoT )

The term "Internet of things" (IoT) refers to a category of devices that are equipped with sensors, software, processing power, and other technologies that allow them to communicate and share data with other devices and systems over the Internet or other communications networks.Electronics, communication, and computer science and engineering are all included in the Internet of things. It has been said that the term "internet of things" is misleading because gadgets just need to be individually addressable and connected to a network, not the whole internet.

### 6. Automated Machine Learning
The technique of automating machine learning applications to real-world issues is known as automated machine learning, or AutoML.

Every step of the process, from starting with a raw dataset to creating a machine learning model that is prepared for deployment, may be included in autoML. The use of machine learning is becoming more and more challenging, and autoML was suggested as an artificial intelligence-based solution. With a high level of automation, AutoML seeks to make machine learning models and techniques accessible to non-experts without necessitating their expertise in the field. There are further benefits to automating the end-to-end machine learning process, such as simpler solutions, faster solution generation, and models that perform better than hand-designed models.


Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.

### 7. Big Data 

Big data is the term used to describe data sets that are too big or complicated for conventional data-processing application software to handle. While data with more features or columns and higher complexity may result in a higher false discovery rate, data with many entries (rows) give greater statistical power. The best interpretation is that it is a vast body of information that cannot be understood when used sparingly, even though it is occasionally used loosely due to a lack of official definition.
